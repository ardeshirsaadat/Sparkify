# Project: Data Modeling with Postgres

## Introduction

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. The analytics team is particularly interested in understanding what songs users are listening to. Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

They'd like a data engineer to create a Postgres database with tables designed to optimize queries on song play analysis, and bring me on the project. My role is to create a database schema and ETL pipeline for this analysis. I'll be able to test the database and ETL pipeline by running queries given to me by the analytics team from Sparkify and compare myur results with their expected results.

## Project Description

In this project, I'll apply data the modeling framework with Postgres and build an ETL pipeline using Python. To complete the project, I will need to define fact and dimension tables for a star schema for a particular analytic focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres using Python and SQL.

## Project Datasets

There are two datasets used in this project.

### Song Dataset

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

### Log Dataset

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Files in the Repository

There are five files in this repository.

### Sql_queries.py

This python script drops any existing tables on the database, and creates five tables to analyze data on the Sparkify App. These five tables are songplays, songs, users, artists and time. This script also contains queries to insert the data from the songs dataset and log dataset into those fact and dimension tables.

### Create_tables.py

This python script helps create the sparkify database and helps connect to the database. It also goes on to create the tables from the sql_queries.py file in the sparkify database. However no data has been inserted into the tables at this point.

This script can be run in the terminal using the code: `python create_tables.py` on the command line.

### Etl.ipynb

The ETL notebook helps build code which is to be used to build pipelines to extract the songs and logs data into the five tables. This file is used in collaboration with the test.ipynb to check that data from the two datasets can be inserted into the five tables.

### Etl.py

This python script connects to the sparkify database and extracts the data from the song and logs database and loads the relevant data into the tables. The code used in this file has been extracted from the etl.ipynb file.

This script can be run in the terminal using the code: `python etl.py` on the command line.

### Test.ipynb

This file is a testing file which is used to check that the tables being created via the other four files is appropriate and correct. The file contains codes which are used as checks to ensure the validity of the tables.

### Purpose of this Database Design

The star schema has been used as it enables fast aggregations and the ability to use joins enables the use of simplified queries. This design also allows for denormalised data which will make it easy for the team at sparkify to access the data across the various tables.

## Running the Python Scripts

A local environment will need to be setup for Postgres to be able to run the python scripts to create the databases/tables and run the etl pipeline to insert the data into those tables.

Here is the link to the [Postgres Documentation](https://www.postgresql.org/docs/current/runtime-config.html) which details how to setup the environment on your local computer.

Once the local environment has been setup you should be able to run the two python scripts using the commands:

    python create_tables.py
    python etl.py
